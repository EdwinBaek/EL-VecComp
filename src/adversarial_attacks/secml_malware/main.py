import os
import yaml
import magic
import numpy as np
from secml.array import CArray
from typing import Tuple, Dict, List

from secml_malware.models.malconv import MalConv
from secml_malware.models.c_classifier_end2end_malware import CClassifierEnd2EndMalware, End2EndModel
from src.models import EL_VecComp, ELAMD, PANACEA

import torch
import torch.nn as nn


def load_EL_VecComp_model(model_path=''):
    # 1. 설정 파일 로드
    with open("ELVecComp_config.yaml", 'r') as f:
        config = yaml.safe_load(f)

    # 2. 모델 생성
    model = EL_VecComp.Model(config)

    # 3. 학습된 가중치 로드 (학습된 모델이 있다고 가정)
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    return model


class ELVecCompAttack:
    def __init__(self, model: nn.Module, epsilon: float = 0.3, alpha: float = 0.1, iterations: int = 10, target: int = None):
        """
            EL-VecComp 모델에 대한 White-box Adversarial Attack 구현

            Args:
                model: 대상 모델 (EL_VecComp.Model)
                epsilon: 최대 perturbation 크기
                alpha: 각 iteration의 step size
                iterations: 공격 반복 횟수
                target: 목표 클래스 (None인 경우 untargeted attack 수행)
        """
        model.eval()
        self.model = model
        self.epsilon = epsilon
        self.alpha = alpha
        self.iterations = iterations
        self.target = target
        self.device = next(model.parameters()).device
        self.confidences_ = []

    def _compute_loss(self, outputs: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """손실 함수 계산"""
        if self.target is not None:
            # Targeted attack: 목표 클래스의 확률을 최대화
            target = torch.ones_like(labels) * self.target
            return -nn.CrossEntropyLoss()(outputs, target)
        else:
            # Untargeted attack: 원래 클래스의 확률을 최소화
            return nn.CrossEntropyLoss()(outputs, labels)

    def perturb( self, x: Dict[str, torch.Tensor], labels: torch.Tensor) -> Tuple[Dict[str, torch.Tensor], List[float]]:
        """
            Adversarial example 생성

            Args:
                x: 입력 feature dictionary
                labels: 원본 레이블

            Returns:
                perturbed_x: 공격된 feature dictionary
                confidences: 각 iteration의 confidence 값 리스트
        """
        # 모델을 eval 모드로 설정
        self.model.eval()

        # 입력을 device로 이동
        x = {k: v.to(self.device) for k, v in x.items()}
        labels = labels.to(self.device)

        # 변수 초기화
        perturbed_x = {k: v.clone().detach().requires_grad_(True) for k, v in x.items()}
        confidences = []

        for i in range(self.iterations):
            # Forward pass
            outputs = self.model(perturbed_x)
            loss = self._compute_loss(outputs, labels)

            # Confidence 기록
            with torch.no_grad():
                confidence = torch.softmax(outputs, dim=1)
                if self.target is not None:
                    conf_value = confidence[:, self.target].mean().item()
                else:
                    conf_value = confidence[range(len(labels)), labels].mean().item()
                confidences.append(conf_value)

            # Backward pass
            loss.backward()

            # FGSM update for each feature
            with torch.no_grad():
                for feature_name in perturbed_x:
                    if perturbed_x[feature_name].grad is not None:
                        perturbation = self.alpha * perturbed_x[feature_name].grad.sign()
                        perturbed_x[feature_name] = perturbed_x[feature_name] + perturbation

                        # Epsilon 제약조건 적용
                        delta = perturbed_x[feature_name] - x[feature_name]
                        delta = torch.clamp(delta, -self.epsilon, self.epsilon)
                        perturbed_x[feature_name] = x[feature_name] + delta

                        # Feature value 범위 제약 (0~1)
                        perturbed_x[feature_name] = torch.clamp(perturbed_x[feature_name], 0, 1)

                        # Gradient 초기화
                        perturbed_x[feature_name].grad.zero_()

        self.confidences_ = confidences
        return perturbed_x, confidences

    def run(self, x: Dict[str, torch.Tensor], labels: torch.Tensor) -> Tuple[torch.Tensor, float, Dict[str, torch.Tensor], float]:
        """
            Attack 실행

            Args:
                x: 입력 feature dictionary
                labels: 원본 레이블

            Returns:
                predictions: 공격된 샘플의 예측값
                attack_success_rate: 공격 성공률
                perturbed_x: 공격된 feature dictionary
                final_confidence: 최종 confidence 값
        """
        perturbed_x, confidences = self.perturb(x, labels)

        with torch.no_grad():
            outputs = self.model(perturbed_x)
            predictions = torch.argmax(outputs, dim=1)

            if self.target is not None:
                attack_success_rate = (predictions == self.target).float().mean().item()
            else:
                attack_success_rate = (predictions != labels).float().mean().item()

        return predictions, attack_success_rate, perturbed_x, confidences[-1]


class ELVecCompDefense:
    def __init__(
            self,
            model: nn.Module,
            epsilon: float = 0.3
    ):
        """
        ELVecComp 모델에 대한 Adversarial Defense 구현

        Args:
            model: 대상 모델 (EL_VecComp.Model)
            epsilon: adversarial training에 사용할 perturbation 크기
        """
        self.model = model
        self.epsilon = epsilon
        self.device = next(model.parameters()).device

    def adversarial_training_step(
            self,
            x: Dict[str, torch.Tensor],
            labels: torch.Tensor,
            optimizer: torch.optim.Optimizer
    ) -> float:
        """
        Adversarial Training 단계 수행

        Args:
            x: 입력 feature dictionary
            labels: 레이블
            optimizer: 옵티마이저

        Returns:
            loss: 학습 손실값
        """
        self.model.train()

        # 일반 학습
        outputs = self.model(x)
        clean_loss = nn.CrossEntropyLoss()(outputs, labels)

        # FGSM 공격 생성
        attack = ELVecCompAttack(
            model=self.model,
            epsilon=self.epsilon,
            iterations=1
        )
        perturbed_x, _ = attack.perturb(x, labels)

        # Adversarial 샘플에 대한 학습
        adv_outputs = self.model(perturbed_x)
        adv_loss = nn.CrossEntropyLoss()(adv_outputs, labels)

        # 전체 손실 계산 및 역전파
        total_loss = 0.5 * (clean_loss + adv_loss)
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        return total_loss.item()


if __name__ == "__main__":
    target_model = load_EL_VecComp_model()

    # Attack 실행
    attack = ELVecCompAttack(model=target_model, epsilon=0.3, alpha=0.1, iterations=10)

    # 5. 데이터셋 로드
    train_loader, valid_loader, test_loader = create_data_loaders(config)

    # 6. 공격 실행
    for batch_x, batch_y in test_loader:
        predictions, success_rate, perturbed_x, final_conf = attack.run(batch_x, batch_y)
        print(f"Attack Success Rate: {success_rate:.4f}")
        print(f"Final Confidence: {final_conf:.4f}")

    predictions, success_rate, perturbed_x, final_conf = attack.run(x, labels)
    print(f"Attack Success Rate: {success_rate:.4f}")
    print(f"Final Confidence: {final_conf:.4f}")

    # Defense 적용
    defense = ELVecCompDefense(model=model, epsilon=0.3)
    optimizer = torch.optim.Adam(model.parameters())

    for epoch in range(num_epochs):
        loss = defense.adversarial_training_step(x, labels, optimizer)
        print(f"Epoch {epoch}, Loss: {loss:.4f}")